<head><title></title><meta charset="utf-8"><link rel="stylesheet" href="./styles.css"></head><body class="light-theme blue_sky" style="padding: 20px;"><div class="shine-editor"><h2>02 Pandas</h2><div><div class="shine-collapsible-section"><div class="shine-section-title-wrapper"><div data-upnote-placeholder-key="title" class="shine-section-title shine-placeholder"><div class="shine-section-title-inner"><h3>Trading View</h3></div></div></div><div data-upnote-placeholder-key="content" class="shine-section-content shine-placeholder"><div class="shine-section-content-inner"><div>This code use basic pandas functions</div><div><pre data-code-language="python" spellcheck="false"># !pip install pandas<br>import pandas as pd<br>columns_to_read = ['Ticker', 'Price', 'Change %', 'Volume', 'Technical Rating']<br><br># Read the CSV file<br>df_csv = pd.read_csv('/Users/larry/Downloads/america_2023-11-12.csv', usecols=columns_to_read)<br><br># Calculate 'Volume*Price'<br>df_csv['Volume*Price'] = df_csv['Volume'] * df_csv['Price']<br><br># Rename columns<br>df_renamed = df_csv.rename(columns={'Change %': 'PctChg', 'Volume*Price': 'VolPrice', 'Technical Rating': 'TechRating'}) has<br><br># Filter by VolPrice and Price<br>df_filtered = df_renamed[(df_renamed['VolPrice'] &gt; 1000000) &amp; (df_renamed['Price'] &gt; 50)]<br><br># Sort<br>df_sorted = df_filtered.sort_values(by=['PctChg'], ascending=False)<br><br># Display the sorted DataFrame<br>print(df_sorted)</pre><div><br></div></div><br></div></div></div><div><br></div></div><div><div class="shine-collapsible-section"><div class="shine-section-title-wrapper"><div data-upnote-placeholder-key="title" class="shine-section-title shine-placeholder"><div class="shine-section-title-inner"><h3>One liners</h3></div></div></div><div data-upnote-placeholder-key="content" class="shine-section-content shine-placeholder"><div class="shine-section-content-inner"><div><a spellcheck="false" class="shine-break-all" href="https://medium.com/python-in-plain-english/efficient-data-manipulation-with-pandas-one-liners-in-python-cf792a855dc7">https://medium.com/python-in-plain-english/efficient-data-manipulation-with-pandas-one-liners-in-python-cf792a855dc7</a><br></div><div><br></div><div><div><br></div><div><div>1. Read an Excel file</div><pre spellcheck="false">import pandas as pd<br>df = pd.read_excel('file.xlsx')</pre><div>2. Write a DataFrame to Excel</div><pre spellcheck="false">df.to_excel('output.xlsx', index=False)</pre><div>3. Read a specific sheet from an Excel file</div><pre spellcheck="false">df = pd.read_excel('file.xlsx', sheet_name='Sheet1')</pre><div>4. Write multiple DataFrames to separate sheets in Excel</div><pre spellcheck="false">with pd.ExcelWriter('output.xlsx') as writer:<br>    df1.to_excel(writer, sheet_name='Sheet1', index=False)<br>    df2.to_excel(writer, sheet_name='Sheet2', index=False)</pre><div>5. Filter rows based on a condition</div><pre spellcheck="false">filtered_df = df[df['Column'] &gt; 10]</pre><div>6. Select specific columns</div><pre spellcheck="false">selected_columns = df[['Column1', 'Column2']]</pre><div>7. Sort DataFrame by a column</div><pre spellcheck="false">sorted_df = df.sort_values(by='Column')</pre><div>8. Count unique values in a column</div><pre spellcheck="false">unique_count = df['Column'].nunique()</pre><div>9. Concatenate two DataFrames vertically</div><pre spellcheck="false">merged_df = pd.concat([df1, df2], axis=0)</pre><div>10. Concatenate two DataFrames horizontally</div><pre spellcheck="false">merged_df = pd.concat([df1, df2], axis=1)</pre><div>11. Fill missing values with a specific value</div><pre spellcheck="false">df_filled = df.fillna(0)</pre><div>12. Drop rows with missing values</div><pre spellcheck="false">df_no_na = df.dropna()</pre><div>13. Rename columns</div><pre spellcheck="false">df = df.rename(columns={'OldName': 'NewName'})</pre><div>14. Create a new column based on a condition</div><pre spellcheck="false">df['NewColumn'] = np.where(df['Column'] &gt; 10, 'High', 'Low')</pre><div>15. Calculate the cumulative sum of a column</div><pre spellcheck="false">df['CumulativeSum'] = df['Column'].cumsum()</pre><div>16. Remove duplicate rows</div><pre spellcheck="false">df_no_duplicates = df.drop_duplicates()</pre><div>17. Merge two DataFrames by a common column</div><pre spellcheck="false">merged_df = pd.merge(df1, df2, on='CommonColumn')</pre><div>18. Convert DataFrame to a NumPy array</div><pre spellcheck="false">numpy_array = df.to_numpy()</pre><div>19. Filter rows based on multiple conditions</div><pre spellcheck="false">filtered_df = df[(df['Column1'] &gt; 10) &amp; (df['Column2'] == 'Value')]</pre><div>20. Calculate the sum of a column</div><pre spellcheck="false">sum_value = df['Column'].sum()</pre><div>21. Pivot table with rows and columns</div><pre spellcheck="false">pivot_table = df.pivot_table(index='RowColumn', columns='ColumnHeader', values='Value', aggfunc='mean')</pre><div>22. Convert Excel date format to a datetime object</div><pre spellcheck="false">df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')</pre><div>23. Create a histogram from a column</div><pre spellcheck="false">df['Column'].plot.hist(bins=20)</pre><div>24. Apply a function to each element in a column</div><pre spellcheck="false">df['ModifiedColumn'] = df['Column'].apply(lambda x: x * 2)</pre><div>25. Count occurrences of unique values in a column</div><pre spellcheck="false">value_counts = df['Column'].value_counts()</pre><div>26. Convert categorical column to one-hot encoding</div><pre spellcheck="false">df_encoded = pd.get_dummies(df, columns=['CategoricalColumn'])</pre><div>27. Calculate the percentage change between consecutive rows</div><pre spellcheck="false">df['PercentageChange'] = df['Column'].pct_change() * 100</pre><div>28. Create a rolling average of a column</div><pre spellcheck="false">df['RollingAverage'] = df['Column'].rolling(window=3).mean()</pre><div>29. Select rows based on a list of values</div><pre spellcheck="false">selected_rows = df[df['Column'].isin(['Value1', 'Value2'])]</pre><div>30. Convert column to lowercase</div><pre spellcheck="false">df['Column'] = df['Column'].str.lower()</pre><div>31. Count the number of null values in each column</div><pre spellcheck="false">null_count = df.isnull().sum()</pre><div>32. Rename all columns to lowercase</div><pre spellcheck="false">df.columns = df.columns.str.lower()</pre><div>33. Remove leading and trailing whitespaces from strings in a column</div><pre spellcheck="false">df['Column'] = df['Column'].str.strip()</pre><div>34. Combine two columns into a new one</div><pre spellcheck="false">df['Combined'] = df['Column1'].astype(str) + df['Column2']</pre><div>35. Transpose the DataFrame</div><pre spellcheck="false">transposed_df = df.transpose()</pre><div>36. Convert a column to a different data type</div><pre spellcheck="false">df['NumericColumn'] = df['NumericColumn'].astype(float)</pre><div>37. Apply a custom function to each group in a grouped DataFrame</div><pre spellcheck="false">result_df = df.groupby('GroupColumn').apply(lambda group: custom_function(group))</pre><div>38. Detect and drop duplicate columns</div><pre spellcheck="false">df = df.loc[:, ~df.columns.duplicated()]</pre><div>39. Convert a column to a categorical variable with custom labels</div><pre spellcheck="false">df['Category'] = pd.cut(df['NumericColumn'], bins=[0, 10, 20, 30], labels=['Low', 'Medium', 'High'])</pre><div>40. Convert a DataFrame to a dictionary</div><pre spellcheck="false">df_dict = df.to_dict()</pre><div>41. Apply a function element-wise to all columns</div><pre spellcheck="false">df = df.applymap(lambda x: custom_function(x) if pd.notnull(x) else x)</pre><div>42. Convert a DataFrame to a Markdown table</div><pre spellcheck="false">markdown_table = df.to_markdown()</pre><div>43. Create a lagged column (shift values down) in a time series</div><pre spellcheck="false">df['LaggedColumn'] = df['Column'].shift(1)</pre><div>44. Calculate the correlation matrix for all numeric columns:</div><pre spellcheck="false">correlation_matrix = df.corr()</pre><div>45. Filter rows where a column contains a substring</div><pre spellcheck="false">filtered_df = df[df['Column'].str.contains('substring')]</pre><div>46. Merge DataFrames using an outer join</div><pre spellcheck="false">merged_df = pd.merge(df1, df2, on='CommonColumn', how='outer')</pre><div>47. Select top N rows with the highest values in a column</div><pre spellcheck="false">top_rows = df.nlargest(5, 'Column')</pre><div>48. Calculate the cumulative product of a column</div><pre spellcheck="false">df['CumulativeProduct'] = df['Column'].cumprod()</pre><div>49. Create a pivot table with multiple aggregation functions</div><pre spellcheck="false">pivot_table = df.pivot_table(index='RowColumn', columns='ColumnHeader', values='Value', aggfunc=['mean', 'sum'])</pre><div>50. Convert column with strings to a datetime object</div><pre spellcheck="false">df['Date'] = pd.to_datetime(df['Date'], errors='coerce')</pre><div>51. Split a column into multiple columns based on a delimiter</div><pre spellcheck="false">df[['First', 'Last']] = df['FullName'].str.split(' ', expand=True)</pre><div>52. Apply a function to each group in a grouped DataFrame and concatenate the results</div><pre spellcheck="false">result_df = pd.concat([group.apply(custom_function) for _, group in df.groupby('GroupColumn')])</pre><div>53. Remove rows where any column is null</div><pre spellcheck="false">df_no_nulls = df.dropna(how='any')</pre><div>54. Convert Unix timestamp to a datetime object</div><pre spellcheck="false">df['Datetime'] = pd.to_datetime(df['UnixTimestamp'], unit='s')</pre><div>55. Filter rows where a column’s values are between a range</div><pre spellcheck="false">filtered_df = df[df['NumericColumn'].between(10, 20)]</pre><div>56. Stack a DataFrame (move innermost column index level to innermost row index level)</div><pre spellcheck="false">stacked_df = df.stack()</pre><div>57. Convert a DataFrame to a JSON file</div><pre spellcheck="false">df.to_json('output.json', orient='records')</pre><div>58. Remove rows with duplicate values in a specific column</div><pre spellcheck="false">df_no_duplicates = df.drop_duplicates(subset='Column')</pre><div>59. Apply a custom function element-wise to a column</div><pre spellcheck="false">df['ModifiedColumn'] = df['Column'].apply(custom_function)</pre><div>60. Check for null values in the entire DataFrame</div><pre spellcheck="false">null_check = df.isnull().any().any()</pre></div><div><h1><br></h1></div></div></div></div></div><div><br></div></div><div><div class="shine-collapsible-section shine-section-collapsed"><div class="shine-section-title-wrapper"><div data-upnote-placeholder-key="title" class="shine-section-title shine-placeholder"><div class="shine-section-title-inner"><h3>Hidden Gems by Eryk Lewinson</h3></div></div></div><div data-upnote-placeholder-key="content" class="shine-section-content shine-placeholder"><div class="shine-section-content-inner"><a spellcheck="false" class="shine-break-all" href="https://medium.com/towards-data-science/level-up-your-pandas-game-with-these-15-hidden-gems-1c6aded2060f">https://medium.com/towards-data-science/level-up-your-pandas-game-with-these-15-hidden-gems-1c6aded2060f</a><br><div><h1 data-testid="storyTitle"><br></h1></div><div>What I genuinely love about&nbsp;<code spellcheck="false">pandas</code>&nbsp;is that you can spend years working with it, and there are probably still quite a few useful methods that you are not aware of. That is why, in the fourth part of the series, I will show you a few more methods you might not have heard about but which will definitely be useful for data wrangling. Before we jump into it, you might want to check out the previous parts of the series:</div><ul><li><a href="https://towardsdatascience.com/9-useful-pandas-methods-you-probably-have-not-heard-about-28ff6c0bceee">9 Useful Pandas Methods You Might Have Not Heard About</a></li><li><a href="https://towardsdatascience.com/8-more-useful-pandas-functionalities-for-your-analyses-ef87dcfe5d74">8 More Useful Pandas Functionalities For Your Analyses</a></li><li><a href="https://towardsdatascience.com/11-useful-pandas-functionalities-you-might-have-overlooked-ad080527c768">11 Useful Pandas Functionalities You Might Have Overlooked</a></li></ul><div>We will cover the methods in alphabetical order, not in terms of their usefulness. Let’s jump right into it!</div><h1>Setup</h1><div>This time, we won’t need any complex libraries, so we will just import the basics:</div><div></div><div>To ensure you can follow along, we are using&nbsp;<code spellcheck="false">pandas</code>&nbsp;2.2.0, which is the latest version available at the time of writing this article.</div><h1>agg</h1><div>You are probably already familiar with performing aggregations in&nbsp;<code spellcheck="false">pandas</code>&nbsp;using methods such as&nbsp;<code spellcheck="false">sum</code>&nbsp;or&nbsp;<code spellcheck="false">min</code>. You have also probably used these methods in combination with&nbsp;<code spellcheck="false">groupby</code>. Therefore, it will not come as a surprise that the&nbsp;<code spellcheck="false">agg</code>&nbsp;method is used to perform one or more aggregations on a DataFrame. What is interesting is that we can use&nbsp;<code spellcheck="false">agg</code>&nbsp;in a few ways, depending on the syntax we use. Let’s illustrate this with some examples.</div><div>By passing a dictionary to the&nbsp;<code spellcheck="false">agg</code>&nbsp;method, we indicate which aggregations (sum, mean, max, etc.) we want to calculate for each column of the DataFrame. The keys of the dictionary represent the columns on which we want to perform the aggregations, while the values represent the operations we want to execute.</div><div></div><div>We are not restricted to passing only a single operation for column either. In the next snippet, we indicate that we want to calculate both the sum and mean using column A.</div><div></div><div>In the resulting DataFrame, we can observe some&nbsp;<code spellcheck="false">NaN</code>&nbsp;values, corresponding to combinations of columns and operations that we have not requested in our dictionary.</div><div>For the next example, let’s make it a bit more complex and add a grouping column. This time, we would like to calculate the mean and sum for all three columns within each of the two groups.</div><div></div><div>Lastly, we use a slightly different syntax to create new columns with names of our choosing. Naturally, we also indicate which operation we want to execute on each column, all within the groups specified with the&nbsp;<code spellcheck="false">groupby</code>&nbsp;clause.</div><div></div><h1>assign</h1><div>The&nbsp;<code spellcheck="false">assign</code>&nbsp;method is used to create a new DataFrame with additional columns, assigning values based on existing columns or operations.</div><div></div><div>Unless we assign the last operation to an object, the DataFrame&nbsp;<code spellcheck="false">df</code>&nbsp;is not modified in place.</div><div>I find this method most useful within chained operations, where we are not necessarily interested in the intermediate steps. For example, we would like to aggregate based on the new column, but we do not want to add it to the original DataFrame.</div><div></div><h1>combine_first</h1><div>The&nbsp;<code spellcheck="false">combine_first</code>&nbsp;method is used to combine two Series (or columns in a DataFrame), choosing values from the first Series and filling in any missing values with the corresponding values from the second Series.</div><div>The&nbsp;<code spellcheck="false">combine_first</code>&nbsp;method in&nbsp;<code spellcheck="false">pandas</code>&nbsp;works similarly to the&nbsp;<code spellcheck="false">COALESCE</code>&nbsp;function in SQL.</div><div></div><div>As we can see, the missing values from the&nbsp;<code spellcheck="false">s1</code>&nbsp;Series were filled in with the values from the&nbsp;<code spellcheck="false">s2</code>&nbsp;Series. If the second series had a missing value, we are left with a missing value.</div><div>Naturally, this method can also be used with columns of a DataFrame. Let’s convert our two Series into a DataFrame and illustrate how the&nbsp;<code spellcheck="false">combine_first</code>&nbsp;method works.</div><div></div><div>We can also chain the&nbsp;<code spellcheck="false">combine_first</code>methods if we want to use more than 2 columns for the potential updating of the missing values.</div><div></div><div>As we can see, the last missing value is now gone, as the value is picked up from the third Series.</div><h1>cumsum / cummin / cummax / cumprod</h1><div>While you might have heard about the&nbsp;<code spellcheck="false">cumsum</code>&nbsp;method, I’d say the chances are high that you have never used the other three. In short, the&nbsp;<code spellcheck="false">cumsum</code>,&nbsp;<code spellcheck="false">cummin</code>,&nbsp;<code spellcheck="false">cummax</code>, and&nbsp;<code spellcheck="false">cumprod</code>&nbsp;methods are used to calculate cumulative sums, minimums, maximums, and products for elements in a Series or DataFrame.</div><div></div><div><em>Pro tip:</em>&nbsp;Those methods can also be used together with&nbsp;<code spellcheck="false">groupby</code>!</div><h1>cut / qcut</h1><div>Let’s start with the simpler one. By default, the&nbsp;<code spellcheck="false">cut</code>&nbsp;function divides the data into bins of equal width. This means that each bin has the same range, but the number of data points in each bin may vary. We can observe this behavior below, where we divide the years of experience into three bins.</div><div></div><div>As we can see in the output, the&nbsp;<code spellcheck="false">cut</code>&nbsp;function categorized all the observations into one of three bins of equal length (approximately 3.33 in length).</div><div>One thing that might require a bit of a refresher here is the&nbsp;<em>interval notation</em>. Let’s look at some hypothetical numbers to understand the notation:</div><ul><li>(0, 10): The interval is not closed on either side, meaning it does not include neither 0 nor 10.</li><li>[0, 10]: The interval is closed on both sides, meaning it includes both 0 and 10.</li><li>[0, 10): The interval is closed on the left side, meaning 0 is included, but 10 is not included.</li><li>(0, 10]: The interval is closed on the right side, meaning 0 is not included, but 10 is included.</li></ul><div>We have already covered the simplest use case of&nbsp;<code spellcheck="false">cut</code>. As the next step, we can also provide custom bin edges. In the snippet below, we define the three seniority levels ourselves. As you can see, when defining the edges ourselves, the resulting bins do not have to be of equal width.</div><div></div><div>We can see one&nbsp;<code spellcheck="false">NaN</code>&nbsp;value in the output. That is because, by definition, the lowest value is not included in the range. We can fix that by setting&nbsp;<code spellcheck="false">include_lowest</code>&nbsp;to&nbsp;<code spellcheck="false">True</code>.</div><div></div><div>And to wrap it up, we can also assign custom label names to our bins. As we split the years of experience, let’s assign seniority labels to each of the bins.</div><div></div><div>We have covered the simpler variant, now let’s look into&nbsp;<code spellcheck="false">qcut</code>. The&nbsp;<code spellcheck="false">qcut</code>&nbsp;function divides the data into bins based on quantiles. This ensures that each bin has approximately the same number of data points. In the snippet below, we indicate that we want to split the data based on three quantiles.</div><div></div><div>As we can see in the output, the 3 bins created using&nbsp;<code spellcheck="false">qcut</code>&nbsp;do not have the same width, but exactly 2 observations fall into each of the bins. Similar to&nbsp;<code spellcheck="false">cut</code>, we can assign label names.</div><div>In summary,&nbsp;<code spellcheck="false">cut</code>&nbsp;divides data into bins of equal width, while&nbsp;<code spellcheck="false">qcut</code>&nbsp;divides data into bins based on quantiles, ensuring roughly equal-sized bins in terms of the number of data points.</div><h1>duplicated / drop_duplicates</h1><div>The&nbsp;<code spellcheck="false">duplicated</code>&nbsp;method returns a boolean Series indicating whether each element in a DataFrame is duplicated (<code spellcheck="false">True</code>) or not (<code spellcheck="false">False</code>).</div><div>In the following example, we create a DataFrame with a total of 4 duplicated observations (2 pairs).</div><div></div><div>Now we can use that flag to filter out duplicated rows or only keep the duplicated ones. Using the default setting (<code spellcheck="false">keep=”first”</code>) of&nbsp;<code spellcheck="false">duplicated</code>, we keep the first matched observation, and all the subsequent ones are marked as duplicates (with&nbsp;<code spellcheck="false">True</code>). Alternatively, we can keep the last duplicated observation using the following snippet:</div><div></div><div>Alternatively, we can mark all the duplicated rows:</div><div></div><div>In this example, we checked complete rows of the DataFrame to see if they are duplicated. Alternatively, we can check for duplicates only in a selection of columns using the&nbsp;<code spellcheck="false">subset</code>&nbsp;argument.</div><div>We have already seen how to mark the duplicated rows. We can also directly drop the duplicates using the&nbsp;<code spellcheck="false">drop_duplicates</code>&nbsp;method.</div><div></div><div>In this method, you can also use the&nbsp;<code spellcheck="false">keep</code>&nbsp;and&nbsp;<code spellcheck="false">subset</code>&nbsp;arguments, just as we have explained them in the case of the&nbsp;<code spellcheck="false">duplicated</code>&nbsp;method.</div><h1>isin</h1><div>The&nbsp;<code spellcheck="false">isin</code>&nbsp;method is used to filter Series and DataFrames by selecting observations where values within a given column are present in a specified list or another Series. The method returns a boolean Series showing us whether each observation in the column is within the specified values.</div><div></div><h1><code spellcheck="false">merge_ordered</code></h1><div>We can use the&nbsp;<code spellcheck="false">merge_ordered</code>&nbsp;function to perform an ordered merge of two DataFrames. This function is particularly useful when we have two DataFrames with sorted keys and we want to merge them while maintaining the order of the keys. This can come in handy when working with time-series data.</div><div></div><div>There are a few things to notice in the snippet:</div><ul><li>On purpose, I have inverted both date columns, but the outcome is still sorted based on the date column in ascending order.</li><li>The default version of the join in&nbsp;<code spellcheck="false">merge_ordered</code>&nbsp;is an outer join (instead of the inner join for&nbsp;<code spellcheck="false">merge</code>).</li><li>We can use the&nbsp;<code spellcheck="false">fill_method</code>&nbsp;to interpolate missing values using one of the available approaches.</li></ul><div>For comparison, you can see below how the combined DataFrame would have looked if we used&nbsp;<code spellcheck="false">merge</code>&nbsp;with an the outer join.</div><div></div><h1>pct_change</h1><div>The&nbsp;<code spellcheck="false">pct_change</code>&nbsp;method is used to calculate the percentage change between the current and a prior element in a Series or DataFrame. Most frequently, it is used to analyze the percentage change in values over time, especially in financial time series data, for example, stock prices.</div><div></div><h1><code spellcheck="false">select_dtypes</code></h1><div><code spellcheck="false">select_dtypes</code>&nbsp;is used to filter columns in a DataFrame based on their data types. We can use it to select columns that have specific data types, such as numeric types (<code spellcheck="false">int64</code>,&nbsp;<code spellcheck="false">float64</code>), object types (<code spellcheck="false">str</code>,&nbsp;<code spellcheck="false">object</code>), boolean or datetime types, etc.</div><div>In the first example, we select the integer and float columns, that is, the numeric ones.</div><div></div><div>Alternatively, we can also select the numeric columns using the&nbsp;<code spellcheck="false">"number"</code>&nbsp;keyword.</div><div></div><div>If we want to select all object types, we can use the&nbsp;<code spellcheck="false">"object"</code>&nbsp;keyword.</div><div></div><div>With&nbsp;<code spellcheck="false">select_dtypes</code>, we can also specify which data types we want to exclude. In the next example, we want to select all columns that do not have an object type.</div><div></div><div><source type="image/webp"><br></div>Yet another panda searching for more hidden gems. Image generated with Midjourney.<h1>Wrapping up</h1><div>I hope you have discovered at least one new functionality of&nbsp;<code spellcheck="false">pandas</code>&nbsp;that will make your data wrangling easier and more fun 🤞 You can find the code used for this article on my&nbsp;<a href="https://github.com/erykml/medium_articles/blob/master/Data%20Wrangling/useful_pandas_4.ipynb">GitHub</a>.</div><div>If you know any other cool or useful&nbsp;<code spellcheck="false">pandas</code>&nbsp;functionalities, please let me know! You can reach out to me on&nbsp;<a href="https://www.linkedin.com/in/eryklewinson/">LinkedIn</a>,&nbsp;<a href="https://twitter.com/erykml1">Twitter</a>, or in the comments.</div><div>Until next time!</div><div>You might also be interested in one of the following:</div><br></div></div></div><div><br></div></div><div><div class="shine-collapsible-section shine-section-collapsed"><div class="shine-section-title-wrapper"><div data-upnote-placeholder-key="title" class="shine-section-title shine-placeholder"><div class="shine-section-title-inner"><h3>Pandas Functions</h3></div></div></div><div data-upnote-placeholder-key="content" class="shine-section-content shine-placeholder"><div class="shine-section-content-inner"><div><a spellcheck="false" class="shine-break-all" href="https://medium.com/@deepaksharma2494/20-pandas-functions-to-complete-80-of-your-data-science-tasks-cefa535a629e">https://medium.com/@deepaksharma2494/20-pandas-functions-to-complete-80-of-your-data-science-tasks-cefa535a629e</a><br></div><div>Let’s delve into the fascinating world of&nbsp;<strong>Data Science</strong>&nbsp;— a realm where art meets science, and playing with data unveils valuable insights crucial for making informed business decisions.</div><div>In my perspective, Data Science is the amalgamation of techniques and tools employed to extract meaningful information from data sets. Imagine&nbsp;<strong>predicting Customer Churn or detecting fraud</strong>&nbsp;— these are just glimpses of what Data Science enables us to achieve.</div><div>To execute such endeavors, various programming languages, including&nbsp;<strong>Python and R</strong>, come to our aid. In the Python ecosystem,&nbsp;<strong>Pandas</strong>&nbsp;stands out as an indispensable library that empowers us to create robust models and handle data efficiently. Additionally, emerging&nbsp;<strong>AI-powered tools</strong>&nbsp;like&nbsp;<strong>Microsoft Fabric</strong>&nbsp;are gaining popularity, but we’ll save that discussion for another story.</div><div>r now, let’s focus on Pandas — a versatile library that has become a cornerstone in many of my projects. Below, I<strong>’ve compiled a list of 20 Pandas functions</strong>&nbsp;that consistently prove invaluable in tackling a multitude of tasks.</div><ol><li><code spellcheck="false">read_csv()</code>: Read a CSV file into a DataFrame.<br>import pandas as pd<br><strong>df = pd.read_csv(‘file.csv’)</strong></li><li><code spellcheck="false">head()</code>: Display the first few rows of the DataFrame.<br><strong>df.head()</strong><br>You can also mention how many rows you want by explicitly passing the number of arguments as below:<br><strong>df.head(5)</strong></li><li><code spellcheck="false">info()</code>: Display a concise summary of the DataFrame, including data types and missing values.<br><strong>df.info()</strong></li><li><code spellcheck="false">describe()</code>: Generate descriptive statistics of the DataFrame.<br><strong>df.describe()</strong></li><li><code spellcheck="false">shape</code>: Get the number of rows and columns in the DataFrame.<br><strong>df.shape</strong></li><li><code spellcheck="false">value_counts()</code>: Count unique values in a column.<br><strong>df[‘column_name’].value_counts()</strong></li><li><code spellcheck="false">isnull()</code>: Check for missing values in the DataFrame<br><strong>df.isnull()</strong></li><li><code spellcheck="false">dropna()</code>: Drop rows with missing values.<br><strong>df.dropna()</strong><br>If there are blank values in the dataframe, they won’t be detected with this. For this, you can use:<br><br><strong>df.dropna(how=’any’, inplace=True)</strong><br>This will drop any row that contains at least one missing value, including blank fields.</li></ol><div>9.&nbsp;<code spellcheck="false">fillna()</code>: Fill missing values with a specified value or strategy.<br><strong>df.fillna(value)</strong></div><div>10.&nbsp;<code spellcheck="false">groupby()</code>: Group DataFrame by a specific column.<br><strong>df.groupby(‘column_name’)</strong></div><div>11.&nbsp;<code spellcheck="false">agg()</code>: Apply multiple aggregation functions to grouped data.<br><strong>df.groupby(‘column_name’).agg([‘mean’, ‘sum’])</strong></div><div>12.&nbsp;<code spellcheck="false">merge()</code>: Merge two DataFrames based on a common column.<br><strong>pd.merge(df1, df2, on=’common_column’)</strong></div><div>13.&nbsp;<code spellcheck="false">concat()</code>: Concatenate two DataFrames along a particular axis.<br><strong>pd.concat([df1, df2], axis=1)</strong></div><div>14.&nbsp;<code spellcheck="false">sort_values()</code>: Sort the DataFrame by one or more columns.<strong><br>df.sort_values(by=’column_name’)</strong></div><div>15.&nbsp;<code spellcheck="false">set_index()</code>: Set the DataFrame index.<br><strong>df.set_index(‘column_name’)</strong></div><div>16.&nbsp;<code spellcheck="false">reset_index()</code>: Reset the DataFrame index.<strong><br>df.reset_index()</strong></div><div>17.&nbsp;<code spellcheck="false">pivot_table()</code>: Create a pivot table based on DataFrame data.<br><strong>pd.pivot_table(df, values=’value’, index=’index_column’, columns=’column_name’)</strong></div><div>18.&nbsp;<code spellcheck="false">apply()</code>: Apply a function along the axis of the DataFrame.<br><strong>df[‘column_name’].apply(function)</strong></div><div>19.&nbsp;<code spellcheck="false">cut()</code>: Bin values into discrete intervals.<br><strong>pd.cut(df[‘column_name’], bins)</strong></div><div>20.&nbsp;<code spellcheck="false">to_csv()</code>: Write the DataFrame to a CSV file.<br><strong>df.to_csv(‘output_file.csv’, index=False)</strong></div><div>Alright, folks, this is the list of functions that I like and are used in almost all types of exploration tasks.</div><div>As I wrap up my first article, I’m aware there may be room for improvement. I’m learning and evolving in this exciting journey through Data Science, and I appreciate your patience as I navigate through it.</div><div>Let’s embark on this learning adventure together. If you’ve found value in what I shared or have suggestions for improvement, I welcome your feedback. Follow me on this journey, and we’ll meet again in future explorations of the vast and ever-evolving world of data.</div><div>Your support means the world to me, and I look forward to connecting with you again soon.</div></div></div></div><div><br></div></div></div></body>